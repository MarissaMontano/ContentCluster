{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# Data Science Project\n",
    "## Clyrical | Basic Architecture \n",
    "\n",
    "\n",
    "\n",
    "##### Marissa Montano\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "**Shortcuts:**  [Dependencies](#dep) | [Helper Functions](#hlp) | [Hypertune](#hyp) | [Analysis](#anal) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Why do you care?\n",
    "\n",
    "\n",
    "\n",
    "   The goal of our project is to classify each songs genre by lyrical content. We will be using a kaggle dataset from Gyanendra Mishra (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics/data) and we will be using the python Sklearn library to actually help us process the data. We want to see what main communities of songs are out there, and what themes/topics and genres usually group together. Hopefully we can find out if most artists fall victim to following a generic “lyrical genre formula” when it comes to writing music.\n",
    "\n",
    "\n",
    "   The methods and techniques we have been using to use to cluster these lyrics are term frequency-inverse document frequency (TF-IDF), word embeddings (Word2vec), singular value decomposition (SVD), K-Means Clustering, Spectral Clustering, Gaussian Mixture Models, K-Nearest Neighbors(KNN), and a Neural Network (NN). We are going to compare two the method of extracting the lyrics to a feature vector (ie. with mapping or embedding) using TF-IDF and Word2vec, then we are going going to (potentially) use the SVD  or PAC reduction method (we might have to reduce our dimensions if our laptops don't have the processing power to cluster with all features), we are going to compare the two clustering methods:K-Means Clustering, Spectral Clustering, against two classification methods: K-nearest, and NN. lastly we are going to try to grab common topics from the classification using LDA.\n",
    "\n",
    "\n",
    "   The goal from this project is to work with natural language processing (NLP). We could have analyzed data from Spotify and just clustered based off of musical measurements like valence, but that seems like a trivial task. We feel like we would get more out of this project this semester if we analyzed the content of the songs (lyrics) instead.\n",
    "   \n",
    "   Hypertune the neural network with gridSearchCV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dep'>\n",
    "    \n",
    "[Back to top](#top)\n",
    "\n",
    "# Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marissa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marissa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#vectorizing\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from gensim.models import Word2Vec # pip install gensim==3.4.0\n",
    "from gensim.models.doc2vec import Doc2Vec,  TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hlp'>\n",
    "    \n",
    "[Back to top](#top)\n",
    "\n",
    "# Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(lyrics):\n",
    "    # Ignore case\n",
    "    lyrics = lyrics.lower()\n",
    "    \n",
    "    # Remove ',!?:. \\n\n",
    "    lyrics = ''.join([word.strip(\",!?:\") for word in lyrics])\n",
    "    lyrics = lyrics.replace('\\n', ' ')\n",
    "    lyrics = lyrics.replace('\\'', '')\n",
    "    \n",
    "    # Remove everything between hard brackets\n",
    "    lyrics = re.sub(pattern=\"[\\(\\[].*?[\\)\\]]\", repl='', string=lyrics)\n",
    "\n",
    "    # Remove x4 and (x4), for example\n",
    "    lyrics = re.sub(pattern=\"(\\()?x\\d+(\\))?\", repl=' ', string=lyrics)\n",
    "    lyrics = lyrics.replace('  ', ' ')\n",
    "    \n",
    "    #remove stop words\n",
    "    lyrics = [w for w in lyrics.split() if not w in stopwords.words('english')] \n",
    "    \n",
    "    #return list of words\n",
    "    return \" \".join(lyrics), lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadData(file_name, sub_list=False):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df.dropna(inplace=True)\n",
    "    if sub_list == True:\n",
    "        #import only 1000 data points because my laptop SUCKS\n",
    "        df = df.head(5000)\n",
    "    # Clean data\n",
    "    df = df.dropna()\n",
    "    #drop Other, Not Avalible \n",
    "    df.drop( df[ df['genre'] == 'Other' ].index , inplace=True)\n",
    "    df.drop( df[ df['genre'] == 'Not Available' ].index , inplace=True)\n",
    "    df.drop( df[ df['genre'] == 'Alkebulan' ].index , inplace=True) \n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        sentense, words = preprocess(df.loc[i,'lyrics'])\n",
    "        df.loc[i,'lyrics'] = sentense\n",
    "        df.loc[i, 'tag'] = TaggedDocument(words=words, tags=[df.loc[i, 'song']+str(i)])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(['genre'], axis=1), df['genre'], test_size=0.20, random_state=42)\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataExploration(x_df, y_df):\n",
    "    \n",
    "    genre_counts = y_df.value_counts()\n",
    "    labels = list(genre_counts.index)\n",
    "    count = list(x_df[\"lyrics\"].apply(lambda x: len(x)))\n",
    "    bwp = [[],[],[],[],[],[],[],[],[],[]]\n",
    "    for i, gen in enumerate(y_df):\n",
    "        bwp[labels.index(gen)].append(count[i])\n",
    "        \n",
    "    fig, ax = plt.subplots(2, figsize=(15,15))\n",
    "    # create pie graph of number of songs per genre\n",
    "    ax[0].pie(genre_counts.values, labels = genre_counts.index, autopct='%1.0f%%')\n",
    "    # create box plot of number of lyrics\n",
    "    ax[1].boxplot(bwp, labels=labels)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first comparsion between tfidf/w2v and spectral and k means\n",
    "def clusterComparison(X_train, X_test, y_train, y_test, n_comp): \n",
    "    #params\n",
    "    n_components = n_comp\n",
    "    n_clusters = y_train.nunique()\n",
    "    \n",
    "     # TFIDF | turn lyrics to vectors\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "    X_tfidf_train = tfidf.fit_transform(X_train['lyrics'])\n",
    "    X_tfidf_test = tfidf.transform(X_test['lyrics'])\n",
    "    \n",
    "    # Word2Vec Doc2vec | turn lyrics to vectors\n",
    "    min_word_count = 3\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    model = Doc2Vec(X_train['tag'].tolist(), workers=num_workers)\n",
    "    X_w2v_train = model.docvecs.vectors_docs\n",
    "    model = Doc2Vec(X_test['tag'].tolist(), workers=num_workers)\n",
    "    X_w2v_test = model.docvecs.vectors_docs\n",
    "\n",
    "    \n",
    "    # SVD | dimension reduction\n",
    "    #svd = TruncatedSVD(n_components=n_components, random_state = 0)\n",
    "    #X_final = svd.fit_transform(X)\n",
    "    \n",
    "    # clusters\n",
    "    kmeans_tf = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans_wv = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    spectral_tf = SpectralClustering(n_clusters=n_clusters, random_state=0)\n",
    "    spectral_wv = SpectralClustering(n_clusters=n_clusters, random_state=0)\n",
    "    models_tf = [kmeans_tf, spectral_tf]\n",
    "    models_wv = [kmeans_wv, spectral_wv]\n",
    "    \n",
    "    pred = []\n",
    "    \n",
    "    for model in models_tf:\n",
    "        # clustering on lyrics\n",
    "        y_pred = model.fit_predict(X_tfidf_train)\n",
    "        df_plot = pd.DataFrame({'Genre':y_train,  'Cluster':y_pred})\n",
    "        pred.append(df_plot)\n",
    "        \n",
    "    for model in models_wv:\n",
    "        # clustering on lyrics\n",
    "        y_pred = model.fit_predict(X_w2v_train)\n",
    "        df_plot = pd.DataFrame({'Genre':y_train,  'Cluster':y_pred})\n",
    "        pred.append(df_plot)\n",
    "\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first comparsion between tfidf/w2v and mlp and knn\n",
    "def classifierComparison(X_train, X_test, y_train, y_test, n_comp): \n",
    "    #params\n",
    "    n_components = n_comp\n",
    "    n_clusters = y_train.nunique()\n",
    "    \n",
    "    # TFIDF | turn lyrics to vectors\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "    X_tfidf_train = tfidf.fit_transform(X_train['lyrics'])\n",
    "    X_tfidf_test = tfidf.transform(X_test['lyrics'])\n",
    "    \n",
    "    # Word2Vec Doc2vec | turn lyrics to vectors\n",
    "    min_word_count = 3\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    model = Doc2Vec(X_train['tag'].tolist(), workers=num_workers)\n",
    "    X_w2v_train = model.docvecs.vectors_docs\n",
    "    model = Doc2Vec(X_test['tag'].tolist(), workers=num_workers)\n",
    "    X_w2v_test = model.docvecs.vectors_docs\n",
    "    \n",
    "    #SVD | dimension reduction\n",
    "    #svd = TruncatedSVD(n_components=n_components, random_state = 0)\n",
    "    #X_final = svd.fit_transform(X_tfidf_train)\n",
    "    \n",
    "    # classifiers\n",
    "    classifiers = []\n",
    "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "    classifiers.append(knn)\n",
    "    nn = MLPClassifier()\n",
    "    classifiers.append(nn)\n",
    "    pred = []\n",
    "    \n",
    "    for clf in classifiers:\n",
    "        # classify on lyrics\n",
    "        clf.fit(X_tfidf_train,y_train)\n",
    "        y_pred = clf.predict(X_tfidf_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        df_plot = pd.DataFrame({'Genre':y_test,  'Cluster':y_pred})\n",
    "        pred.append(df_plot)\n",
    "        \n",
    "    for clf in classifiers:\n",
    "        # classify on lyrics\n",
    "        clf.fit(X_w2v_train,y_train)\n",
    "        y_pred = clf.predict(X_w2v_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        df_plot = pd.DataFrame({'Genre':y_test,  'Cluster':y_pred})\n",
    "        pred.append(df_plot)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hyp'>\n",
    "    \n",
    "[Back to top](#top)\n",
    "\n",
    "# Hypertune the Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertune_nn(X_train, X_test, y_train, y_test, n_comp): \n",
    "    #params\n",
    "    n_components = n_comp\n",
    "    n_clusters = y_train.nunique()\n",
    "    \n",
    "    # TFIDF | turn lyrics to vectors\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "    X_tfidf_train = tfidf.fit_transform(X_train['lyrics'])\n",
    "    X_tfidf_test = tfidf.transform(X_test['lyrics'])\n",
    "    \n",
    "    # Word2Vec Doc2vec | turn lyrics to vectors\n",
    "    min_word_count = 3\n",
    "    num_workers = multiprocessing.cpu_count()\n",
    "    model = Doc2Vec(X_train['tag'].tolist(), workers=num_workers)\n",
    "    X_w2v_train = model.docvecs.vectors_docs\n",
    "    model = Doc2Vec(X_test['tag'].tolist(), workers=num_workers)\n",
    "    X_w2v_test = model.docvecs.vectors_docs\n",
    "    \n",
    "\n",
    "    #TFIDF:\n",
    "    nn = MLPClassifier(max_iter=50)\n",
    "    parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (10,50,10), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "    clf = GridSearchCV(nn, parameter_space, n_jobs=-1, cv=3)\n",
    "    clf.fit(X_tfidf_train,y_train)\n",
    "    print('\\n***\\nBest parameters found in TF-IDF:\\n***\\n', clf.best_params_)\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "    #W2V\n",
    "    nn = MLPClassifier(max_iter=100)\n",
    "    parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "    clf = GridSearchCV(nn, parameter_space, n_jobs=-1, cv=3)\n",
    "    clf.fit(X_w2v_train,y_train)\n",
    "    print('\\n***\\nBest parameters found in Word2Vec:\\n***\\n', clf.best_params_)\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuned nn data (This is after I know TFIDF and MLP work the best)\n",
    "def nn(X_train, X_test, y_train, y_test, labels): \n",
    "    #params\n",
    "    n_clusters = y_train.nunique()\n",
    "    \n",
    "    # TFIDF | turn lyrics to vectors\n",
    "    tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "    X_tfidf_train = tfidf.fit_transform(X_train['lyrics'])\n",
    "    X_tfidf_test = tfidf.transform(X_test['lyrics'])\n",
    "    \n",
    "    # TFIDF with hypertined params\n",
    "    nn = MLPClassifier(max_iter=50, activation='tanh', solver='adam', alpha=0.05, learning_rate='adaptive')\n",
    "    nn.fit(X_tfidf_train,y_train)\n",
    "    y_pred = nn.predict(X_tfidf_test)\n",
    "    print(\"TFIDF and MLP accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    df_plot = pd.DataFrame({'Genre':y_test,  'Cluster':y_pred})\n",
    "    \n",
    "    # Plot normalized confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    title=\"Normalized confusion matrix\"\n",
    "    normalize= \"true\"\n",
    "    disp = plot_confusion_matrix(nn, X_tfidf_test, y_test,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=normalize, ax=ax)\n",
    "    disp.ax_.set_title(title)\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "    plt.show()\n",
    "    \n",
    "    return df_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='anal'>\n",
    "    \n",
    "[Back to top](#top)\n",
    "\n",
    "# Analyze and explore data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data in (if you pass true you only get 5000 data points)\n",
    "X_train, X_test, y_train, y_test = loadData('lyrics.csv', True) \n",
    "# Explore the data through graphs\n",
    "dataExploration(X_train, y_train)\n",
    "\n",
    "n_components = 20\n",
    "\n",
    "# Initial run at cluster and classification comparison\n",
    "acc_class = classifierComparison(X_train, X_test, y_train, y_test, n_components)\n",
    "acc_clust = clusterComparison(X_train, X_test, y_train, y_test, n_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure data for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_counts = y_train.value_counts()\n",
    "labels = list(genre_counts.index)\n",
    "#tf_class_svm = [[],[],[],[],[],[],[],[],[],[]]\n",
    "#wv_class_svm = [[],[],[],[],[],[],[],[],[],[]]\n",
    "tf_class_knn = [[],[],[],[],[],[],[],[],[],[]]\n",
    "wv_class_knn = [[],[],[],[],[],[],[],[],[],[]]\n",
    "tf_class_nn = [[],[],[],[],[],[],[],[],[],[]]\n",
    "wv_class_nn = [[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "tf_clust_kmeans = [[],[],[],[],[],[],[],[],[],[]]\n",
    "tf_clust_spect = [[],[],[],[],[],[],[],[],[],[]]\n",
    "wv_clust_kmeans = [[],[],[],[],[],[],[],[],[],[]]\n",
    "wv_clust_spect = [[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "#for i, row in acc_class[0].iterrows():\n",
    "#    tf_class_svm[labels.index(acc_class[0].loc[i,'Genre'])].append(acc_class[0].loc[i,'Cluster'])\n",
    "for i, row in acc_class[1].iterrows():\n",
    "    tf_class_knn[labels.index(acc_class[1].loc[i,'Genre'])].append(acc_class[1].loc[i,'Cluster'])\n",
    "for i, row in acc_class[2].iterrows():\n",
    "    tf_class_nn[labels.index(acc_class[2].loc[i,'Genre'])].append(acc_class[2].loc[i,'Cluster'])\n",
    "#for i, row in acc_class[3].iterrows():\n",
    "#    wv_class_svm[labels.index(acc_class[3].loc[i,'Genre'])].append(acc_class[3].loc[i,'Cluster'])\n",
    "for i, row in acc_class[4].iterrows():\n",
    "    wv_class_knn[labels.index(acc_class[4].loc[i,'Genre'])].append(acc_class[4].loc[i,'Cluster'])\n",
    "for i, row in acc_class[5].iterrows():\n",
    "    wv_class_nn[labels.index(acc_class[5].loc[i,'Genre'])].append(acc_class[5].loc[i,'Cluster'])\n",
    "\n",
    "    \n",
    "for i, row in acc_clust[0].iterrows():\n",
    "    tf_clust_kmeans[labels.index(acc_clust[0].loc[i,'Genre'])].append(acc_clust[0].loc[i,'Cluster'])\n",
    "for i, row in acc_clust[1].iterrows():  \n",
    "    tf_clust_spect[labels.index(acc_clust[1].loc[i,'Genre'])].append(acc_clust[1].loc[i,'Cluster'])\n",
    "for i, row in acc_clust[2].iterrows():\n",
    "    wv_clust_kmeans[labels.index(acc_clust[2].loc[i,'Genre'])].append(acc_clust[2].loc[i,'Cluster'])\n",
    "for i, row in acc_clust[3].iterrows():  \n",
    "    wv_clust_spect[labels.index(acc_clust[3].loc[i,'Genre'])].append(acc_clust[3].loc[i,'Cluster'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Evaluation of Clusterd data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(labels[i])\n",
    "    #kmeans\n",
    "    plt.subplot(1, 2, 1)\n",
    "    label, counts = np.unique(wv_clust_kmeans[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"purple\", label=\"Word2Vec k-means\")\n",
    "    label, counts = np.unique(tf_clust_kmeans[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"gray\", label=\"TFIDF kmeans\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks(range(len(labels)))\n",
    "    plt.xlim([-1, len(labels)])\n",
    "    # spect\n",
    "    plt.subplot(1, 2, 2)\n",
    "    label, counts = np.unique(wv_clust_spect[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"purple\", label=\"Word2Vec spectral\")\n",
    "    label, counts = np.unique(tf_clust_spect[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"gray\", label=\"TFIDF spectral\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xticks(range(len(labels)))\n",
    "    plt.xlim([-1, len(labels)])\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of classification data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(labels[i])\n",
    "    #knn\n",
    "    plt.subplot(1, 2, 1)\n",
    "    label, counts = np.unique(wv_class_knn[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"purple\", label=\"Word2Vec knn\")\n",
    "    label, counts = np.unique(tf_class_knn[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"gray\", label=\"TFIDF knn\")\n",
    "    plt.legend(loc='upper right')\n",
    "    #nn\n",
    "    plt.subplot(1, 2, 2)\n",
    "    label, counts = np.unique(wv_class_nn[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"purple\", label=\"Word2Vec nn\")\n",
    "    label, counts = np.unique(tf_class_nn[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"gray\", label=\"TFIDF nn\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation of hypertuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypertune params\n",
    "# Don't run unless you have 3 hours to waist \n",
    "hypertune_nn(X_train, X_test, y_train, y_test, n_components)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Analysis of MLP and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Final analysis of mpl and tfidf\n",
    "genre_counts = y_train.value_counts()\n",
    "labels = list(genre_counts.index)\n",
    "final_acc = nn(X_train, X_test, y_train, y_test, labels) \n",
    "\n",
    "# each column is the correct genre, and each row is what the song was actually labed\n",
    "genre_class = [[],[],[],[],[],[],[],[],[],[]]\n",
    "\n",
    "for i, row in final_acc.iterrows():\n",
    "    genre_class[labels.index(final_acc.loc[i,'Genre'])].append(final_acc.loc[i,'Cluster'])\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i])\n",
    "    label, counts = np.unique(genre_class[i], return_counts=True)\n",
    "    plt.bar(label, counts, align='center', alpha=0.5, color=\"purple\")\n",
    "    plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
