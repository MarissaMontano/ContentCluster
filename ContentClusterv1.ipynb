{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Project\n",
    "## Content Cluster | Basic Architecture \n",
    "\n",
    "\n",
    "##### Marissa Montano & Mohamed Al-Rasbi\n",
    "\n",
    "\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do you care?\n",
    "\n",
    "The goal of our project is to cluster songs by lyrical content. We will be using a kaggle dataset from Gyanendra Mishra (https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics/data) and we will be using the python Sklearn library to actually help us process the data. We want to see what main communities of songs are out there, and what  themes usually become popular nowadays. \n",
    "\n",
    "The methods and techniques we are going to use to cluster these lyrics are term frequency-inverse document frequency (TF-IDF), singular value decomposition (SVD), K-Means Clustering. We are going to extract the lyrics to a feature mapping vector using TF-IDF, then we are going to reduce our dimensions with SVD, and lastly we are going to cluster the songs with k-means clustering.  \n",
    "\n",
    "The goal from this project is to work with natural language processing (NLP). We could have analyzed data from Spotify and just clustered based off of musical measurements like valence, but that seems like a trivial task. We feel like we would get more out of this project this semester if we analyzed the content of the songs (lyrics) instead. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback 1:\n",
    " - Questions you want to answer spesifically \n",
    " - Silhouette coefficient or Genre\n",
    " - Word2Vec (gets meaning of lyrics)\n",
    " - Other clustering methods\n",
    "\n",
    "#### Solutions 1:\n",
    " - Does sentiment of lyrics help predict the genre? Do artists fall victim to making things that sell by following a specific/generic formula? \n",
    " - We used kaggle because we wanted genres and Genius didn't have them\n",
    " - We will compare word2vec and tf-idf\n",
    " - We compare our results using three other clustering algorithms\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(lyrics):\n",
    "    # Ignore case\n",
    "    lyrics = lyrics.lower()\n",
    "    \n",
    "    # Remove ',!?:. \\n\n",
    "    lyrics = ''.join([word.strip(\",!?:\") for word in lyrics])\n",
    "    lyrics = lyrics.replace('\\n', ' ')\n",
    "    lyrics = lyrics.replace('\\'', '')\n",
    "    \n",
    "    # Remove everything between hard brackets\n",
    "    lyrics = re.sub(pattern=\"[\\(\\[].*?[\\)\\]]\", repl='', string=lyrics)\n",
    "\n",
    "    # Remove x4 and (x4), for example\n",
    "    lyrics = re.sub(pattern=\"(\\()?x\\d+(\\))?\", repl=' ', string=lyrics)\n",
    "\n",
    "    return(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    oh baby how you doing you know im gonna cut ri...\n",
      "Name: lyrics, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def loadData(file_name, sub_list=False):\n",
    "    df = pd.read_csv(file_name)\n",
    "    if sub_list == True\n",
    "        #import only 1000 data points because my laptop SUCKS\n",
    "        df = df.head(1000)\n",
    "    # Clean data\n",
    "    df = df.dropna()\n",
    "    for i, row in df.iterrows():\n",
    "        df.loc[i,'lyrics'] = preprocess(df.loc[i,'lyrics'])\n",
    "    return df\n",
    "\n",
    "df = loadData('lyrics.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Cluster Genre\n",
      "0        3   Pop\n",
      "1        3   Pop\n",
      "2        4   Pop\n",
      "3        4   Pop\n",
      "4        2   Pop\n"
     ]
    }
   ],
   "source": [
    "# This is me playing around and cluster looking for genre \n",
    "\n",
    "# Need to put hyperperams in yaml file or txt file\n",
    "n_components = 5\n",
    "n_clusters = len(set(df[\"genre\"])) \n",
    "\n",
    "# TFIDF | turn lyrics to vectors\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "X = tfidf.fit_transform(df['lyrics'])\n",
    "\n",
    "# SVD | dimension reduction\n",
    "svd = TruncatedSVD(n_components=n_components, random_state = 0)\n",
    "X_final = svd.fit_transform(X)\n",
    "\n",
    "# K-mean clustering on lyrics\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state = 0)\n",
    "X_clustered = kmeans.fit_predict(X)\n",
    "\n",
    "# display by groups\n",
    "df_plot = pd.DataFrame(list(df[\"genre\"]), list(X_clustered))\n",
    "df_plot = df_plot.reset_index()\n",
    "df_plot.rename(columns = {'index': 'Cluster', 0: 'Genre'}, inplace = True)\n",
    "df_plot['Cluster'] = df_plot['Cluster'].astype(int)\n",
    "\n",
    "print(df_plot.head())\n",
    "#for i, row in df_plot.iterrows():\n",
    "#    print(df_plot.loc[i,'Cluster'],  df_plot.loc[i,'Genre'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is something we should do do a word2vec training and clustering to see genre and lyrical sentiment.\n",
    "# https://github.com/ravishchawla/word_2_vec/blob/master/word_2_vec.ipynb\n",
    "\n",
    "num_features = 100;    # Dimensionality of the hidden layer representation\n",
    "min_word_count = 40;   # Minimum word count to keep a word in the vocabulary\n",
    "num_workers = multiprocessing.cpu_count();       # Number of threads to run in parallel set to total number of cpus.\n",
    "context = 5          # Context window size (on each side)                                                       \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "# Initialize and train the model. \n",
    "#The LineSentence object allows us to pass in a file name directly as input to Word2Vec,\n",
    "#instead of having to read it into memory first.\n",
    "print(\"Training model...\");\n",
    "model = word2vec.Word2Vec(LineSentence('/mnt/big/out_full_clean'), workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling);\n",
    "# We don't plan on training the model any further, so calling \n",
    "# init_sims will make the model more memory efficient by normalizing the vectors in-place.\n",
    "model.init_sims(replace=True);\n",
    "# Save the model\n",
    "model_name = \"model_full_reddit\";\n",
    "model.save(model_name);\n",
    "\n",
    "\n",
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    \n",
    "    return kmeans_clustering.cluster_centers_, idx;\n",
    "\n",
    "model = word2vec.Word2Vec.load('model_full_reddit');\n",
    "Z = model.wv.syn0;\n",
    "\n",
    "centers, clusters = clustering_on_wordvecs(Z, 50);\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters));\n",
    "\n",
    "\n",
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "\n",
    "    #Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "\n",
    "    #Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i+1).zfill(2)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "\n",
    "    #A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
